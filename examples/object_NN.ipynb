{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab5b2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# Authors:                                                                                                     #\n",
    "# Kenny Young (kjyoung@ualberta.ca)                                                                            #\n",
    "# Tian Tian(ttian@ualberta.ca)                                                                                 #\n",
    "#                                                                                                              #\n",
    "# python3 dqn.py -g <game>                                                                                     #\n",
    "#   -o, --output <directory/file name prefix>                                                                  #\n",
    "#   -v, --verbose: outputs the average returns every 1000 episodes                                             #\n",
    "#   -l, --loadfile <directory/file name of the saved model>                                                    #\n",
    "#   -a, --alpha <number>: step-size parameter                                                                  #\n",
    "#   -s, --save: save model data every 1000 episodes                                                            #\n",
    "#   -r, --replayoff: disable the replay buffer and train on each state transition                              #\n",
    "#   -t, --targetoff: disable the target network                                                                #\n",
    "#                                                                                                              #\n",
    "# References used for this implementation:                                                                     #\n",
    "#   https://pytorch.org/docs/stable/nn.html#                                                                   #\n",
    "#   https://pytorch.org/docs/stable/torch.html                                                                 #\n",
    "#   https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html                                   #\n",
    "################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdb7b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "import random, numpy, argparse, logging, os\n",
    "\n",
    "from collections import namedtuple\n",
    "from minatar import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89a28d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# Constants\n",
    "#\n",
    "################################################################################################################\n",
    "BATCH_SIZE = 32\n",
    "REPLAY_BUFFER_SIZE = 100000\n",
    "TARGET_NETWORK_UPDATE_FREQ = 1000\n",
    "TRAINING_FREQ = 1\n",
    "NUM_FRAMES = 5000000\n",
    "FIRST_N_FRAMES = 100000\n",
    "REPLAY_START_SIZE = 5000\n",
    "END_EPSILON = 0.1\n",
    "STEP_SIZE = 0.00025\n",
    "GRAD_MOMENTUM = 0.95\n",
    "SQUARED_GRAD_MOMENTUM = 0.95\n",
    "MIN_SQUARED_GRAD = 0.01\n",
    "GAMMA = 0.99\n",
    "EPSILON = 1.0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b741d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# class QNetwork\n",
    "#\n",
    "# One hidden 2D conv with variable number of input channels.  We use 16 filters, a quarter of the original DQN\n",
    "# paper of 64.  One hidden fully connected linear layer with a quarter of the original DQN paper of 512\n",
    "# rectified units.  Finally, the output layer is a fully connected linear layer with a single output for each\n",
    "# valid action.\n",
    "#\n",
    "################################################################################################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        # One hidden 2D convolution layer:\n",
    "        #   in_channels: variable\n",
    "        #   out_channels: 16\n",
    "        #   kernel_size: 3 of a 3x3 filter matrix\n",
    "        #   stride: 1\n",
    "        self.conv = nn.Conv2d(in_channels, 16, kernel_size=3, stride=1)\n",
    "\n",
    "        # Final fully connected hidden layer:\n",
    "        #   the number of linear unit depends on the output of the conv\n",
    "        #   the output consist 128 rectified units\n",
    "        def size_linear_unit(size, kernel_size=3, stride=1):\n",
    "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "        num_linear_units = size_linear_unit(10) * size_linear_unit(10) * 16\n",
    "        self.fc_hidden = nn.Linear(in_features=num_linear_units, out_features=128)\n",
    "\n",
    "        # Output layer:\n",
    "        self.output = nn.Linear(in_features=128, out_features=num_actions)\n",
    "\n",
    "    # As per implementation instructions according to pytorch, the forward function should be overwritten by all\n",
    "    # subclasses\n",
    "    def forward(self, x):\n",
    "        # Rectified output from the first conv layer\n",
    "        x = f.relu(self.conv(x))\n",
    "\n",
    "        # Rectified output from the final hidden layer\n",
    "        x = f.relu(self.fc_hidden(x.view(x.size(0), -1)))\n",
    "\n",
    "        # Returns the output from the fully-connected linear layer\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac7f65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "# class replay_buffer\n",
    "#\n",
    "# A cyclic buffer of a fixed size containing the last N number of recent transitions.  A transition is a\n",
    "# tuple of state, next_state, action, reward, is_terminal.  The boolean is_terminal is used to indicate\n",
    "# whether if the next state is a terminal state or not.\n",
    "#\n",
    "###########################################################################################################\n",
    "transition = namedtuple('transition', 'state, next_state, action, reward, is_terminal')\n",
    "class replay_buffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.location = 0\n",
    "        self.buffer = []\n",
    "\n",
    "    def add(self, *args):\n",
    "        # Append when the buffer is not full but overwrite when the buffer is full\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(transition(*args))\n",
    "        else:\n",
    "            self.buffer[self.location] = transition(*args)\n",
    "\n",
    "        # Increment the buffer location\n",
    "        self.location = (self.location + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c40041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# get_state\n",
    "#\n",
    "# Converts the state given by the environment to a tensor of size (in_channel, 10, 10), and then\n",
    "# unsqueeze to expand along the 0th dimension so the function returns a tensor of size (1, in_channel, 10, 10).\n",
    "#\n",
    "# Input:\n",
    "#   s: current state as numpy array\n",
    "#\n",
    "# Output: current state as tensor, permuted to match expected dimensions\n",
    "#\n",
    "################################################################################################################\n",
    "def get_state(s):\n",
    "    return (torch.tensor(s, device=device).permute(2, 0, 1)).unsqueeze(0).float()\n",
    "\n",
    "def get_cont_state(cont_s, max_obj=40):\n",
    "    \"\"\"\n",
    "    Return the continuous state of the environment as a torch array.\n",
    "    :param cont_s: Continuous state.\n",
    "    :return: Torch tensor of shape [M*(2+N)]. M is the number of objects. N is the number of categories.\n",
    "    \"\"\"\n",
    "    N = len(cont_s)\n",
    "    obj_len = len(cont_s[0][0])\n",
    "\n",
    "    # Collect all the states\n",
    "    cont_state = []\n",
    "    for i in range(N):\n",
    "        for obj in cont_s[i]:\n",
    "            # Append to the list\n",
    "            assert len(obj) == 9\n",
    "            cont_state.append(torch.tensor(obj, device=device))\n",
    "\n",
    "    # Convert into one torch tensor\n",
    "    cont_state = torch.vstack(cont_state)\n",
    "\n",
    "    # Zero pad to the maximum allowed dimension\n",
    "    size_pad = max_obj - cont_state.shape[0]\n",
    "    pad = torch.zeros((size_pad, obj_len), device=device)\n",
    "    cont_state = torch.cat([cont_state, pad])\n",
    "\n",
    "    # Unsqueeze for the batch dimension\n",
    "    cont_state = cont_state.unsqueeze(0)\n",
    "\n",
    "    return cont_state\n",
    "\n",
    "################################################################################################################\n",
    "# world_dynamics\n",
    "#\n",
    "# It generates the next state and reward after taking an action according to the behavior policy.  The behavior\n",
    "# policy is epsilon greedy: epsilon probability of selecting a random action and 1 - epsilon probability of\n",
    "# selecting the action with max Q-value.\n",
    "#\n",
    "# Inputs:\n",
    "#   t : frame\n",
    "#   replay_start_size: number of frames before learning starts\n",
    "#   num_actions: number of actions\n",
    "#   s: current state\n",
    "#   env: environment of the game\n",
    "#   policy_net: policy network, an instance of QNetwork\n",
    "#\n",
    "# Output: next state, action, reward, is_terminated\n",
    "#\n",
    "################################################################################################################\n",
    "def world_dynamics(t, replay_start_size, num_actions, s_cont, env, policy_net):\n",
    "\n",
    "    # A uniform random policy is run before the learning starts\n",
    "    if t < replay_start_size:\n",
    "        action = torch.tensor([[random.randrange(num_actions)]], device=device)\n",
    "    else:\n",
    "        # Epsilon-greedy behavior policy for action selection\n",
    "        # Epsilon is annealed linearly from 1.0 to END_EPSILON over the FIRST_N_FRAMES and stays 0.1 for the\n",
    "        # remaining frames\n",
    "        epsilon = END_EPSILON if t - replay_start_size >= FIRST_N_FRAMES \\\n",
    "            else ((END_EPSILON - EPSILON) / FIRST_N_FRAMES) * (t - replay_start_size) + EPSILON\n",
    "\n",
    "        if numpy.random.binomial(1, epsilon) == 1:\n",
    "            action = torch.tensor([[random.randrange(num_actions)]], device=device)\n",
    "        else:\n",
    "            # State is 10x10xchannel, max(1)[1] gives the max action value (i.e., max_{a} Q(s, a)).\n",
    "            # view(1,1) shapes the tensor to be the right form (e.g. tensor([[0]])) without copying the\n",
    "            # underlying tensor.  torch._no_grad() avoids tracking history in autograd.\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(s_cont).max(1)[1].view(1, 1)\n",
    "\n",
    "    # Act according to the action and observe the transition and reward\n",
    "    reward, terminated = env.act(action)\n",
    "\n",
    "    # Obtain s_prime\n",
    "    # s_prime = get_state(env.state())\n",
    "    s_cont_prime = get_cont_state(env.continous_state())\n",
    "\n",
    "    return s_cont_prime, action, torch.tensor([[reward]], device=device).float(), torch.tensor([[terminated]], device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbdc6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# train\n",
    "#\n",
    "# This is where learning happens. More specifically, this function learns the weights of the policy network\n",
    "# using huber loss.\n",
    "#\n",
    "# Inputs:\n",
    "#   sample: a batch of size 1 or 32 transitions\n",
    "#   policy_net: an instance of QNetwork\n",
    "#   target_net: an instance of QNetwork\n",
    "#   optimizer: centered RMSProp\n",
    "#\n",
    "################################################################################################################\n",
    "def train(sample, policy_net, target_net, optimizer):\n",
    "    # Batch is a list of namedtuple's, the following operation returns samples grouped by keys\n",
    "    batch_samples = transition(*zip(*sample))\n",
    "\n",
    "    # states, next_states are of tensor (BATCH_SIZE, in_channel, 10, 10) - inline with pytorch NCHW format\n",
    "    # actions, rewards, is_terminal are of tensor (BATCH_SIZE, 1)\n",
    "    states = torch.cat(batch_samples.state)\n",
    "    next_states = torch.cat(batch_samples.next_state)\n",
    "    actions = torch.cat(batch_samples.action)\n",
    "    rewards = torch.cat(batch_samples.reward)\n",
    "    is_terminal = torch.cat(batch_samples.is_terminal)\n",
    "\n",
    "    # Obtain a batch of Q(S_t, A_t) and compute the forward pass.\n",
    "    # Note: policy_network output Q-values for all the actions of a state, but all we need is the A_t taken at time t\n",
    "    # in state S_t.  Thus we gather along the columns and get the Q-values corresponds to S_t, A_t.\n",
    "    # Q_s_a is of size (BATCH_SIZE, 1).\n",
    "    Q_s_a = policy_net(states).gather(1, actions)\n",
    "\n",
    "    # Obtain max_{a} Q(S_{t+1}, a) of any non-terminal state S_{t+1}.  If S_{t+1} is terminal, Q(S_{t+1}, A_{t+1}) = 0.\n",
    "    # Note: each row of the network's output corresponds to the actions of S_{t+1}.  max(1)[0] gives the max action\n",
    "    # values in each row (since this a batch).  The detach() detaches the target net's tensor from computation graph so\n",
    "    # to prevent the computation of its gradient automatically.  Q_s_prime_a_prime is of size (BATCH_SIZE, 1).\n",
    "\n",
    "    # Get the indices of next_states that are not terminal\n",
    "    none_terminal_next_state_index = torch.tensor([i for i, is_term in enumerate(is_terminal) if is_term == 0], dtype=torch.int64, device=device)\n",
    "    # Select the indices of each row\n",
    "    none_terminal_next_states = next_states.index_select(0, none_terminal_next_state_index)\n",
    "\n",
    "    Q_s_prime_a_prime = torch.zeros(len(sample), 1, device=device)\n",
    "    if len(none_terminal_next_states) != 0:\n",
    "        Q_s_prime_a_prime[none_terminal_next_state_index] = target_net(none_terminal_next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "    # Compute the target\n",
    "    target = rewards + GAMMA * Q_s_prime_a_prime\n",
    "\n",
    "    # Huber loss\n",
    "    loss = f.smooth_l1_loss(target, Q_s_a)\n",
    "\n",
    "    # Zero gradients, backprop, update the weights of policy_net\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01df8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "# dqn\n",
    "#\n",
    "# DQN algorithm with the option to disable replay and/or target network, and the function saves the training data.\n",
    "#\n",
    "# Inputs:\n",
    "#   env: environment of the game\n",
    "#   replay_off: disable the replay buffer and train on each state transition\n",
    "#   target_off: disable target network\n",
    "#   output_file_name: directory and file name prefix to output data and network weights, file saved as \n",
    "#       <output_file_name>_data_and_weights\n",
    "#   store_intermediate_result: a boolean, if set to true will store checkpoint data every 1000 episodes\n",
    "#       to a file named <output_file_name>_checkpoint\n",
    "#   load_path: file path for a checkpoint to load, and continue training from\n",
    "#   step_size: step-size for RMSProp optimizer\n",
    "#\n",
    "#################################################################################################################\n",
    "def dqn(env, replay_off, target_off, output_file_name, store_intermediate_result=False, load_path=None, step_size=STEP_SIZE):\n",
    "\n",
    "    # Get channels and number of actions specific to each game\n",
    "    in_channels = env.state_shape()[2]\n",
    "    num_actions = env.num_actions()\n",
    "\n",
    "    # Instantiate networks, optimizer, loss and buffer\n",
    "    policy_net = QNetwork(in_channels, num_actions).to(device)\n",
    "    replay_start_size = 0\n",
    "    if not target_off:\n",
    "        target_net = QNetwork(in_channels, num_actions).to(device)\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    if not replay_off:\n",
    "        r_buffer = replay_buffer(REPLAY_BUFFER_SIZE)\n",
    "        replay_start_size = REPLAY_START_SIZE\n",
    "\n",
    "    optimizer = optim.RMSprop(policy_net.parameters(), lr=step_size, alpha=SQUARED_GRAD_MOMENTUM, centered=True, eps=MIN_SQUARED_GRAD)\n",
    "\n",
    "    # Set initial values\n",
    "    e_init = 0\n",
    "    t_init = 0\n",
    "    policy_net_update_counter_init = 0\n",
    "    avg_return_init = 0.0\n",
    "    data_return_init = []\n",
    "    frame_stamp_init = []\n",
    "\n",
    "    # Load model and optimizer if load_path is not None\n",
    "    if load_path is not None and isinstance(load_path, str):\n",
    "        checkpoint = torch.load(load_path)\n",
    "        policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
    "\n",
    "        if not target_off:\n",
    "            target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
    "\n",
    "        if not replay_off:\n",
    "            r_buffer = checkpoint['replay_buffer']\n",
    "\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        e_init = checkpoint['episode']\n",
    "        t_init = checkpoint['frame']\n",
    "        policy_net_update_counter_init = checkpoint['policy_net_update_counter']\n",
    "        avg_return_init = checkpoint['avg_return']\n",
    "        data_return_init = checkpoint['return_per_run']\n",
    "        frame_stamp_init = checkpoint['frame_stamp_per_run']\n",
    "\n",
    "        # Set to training mode\n",
    "        policy_net.train()\n",
    "        if not target_off:\n",
    "            target_net.train()\n",
    "\n",
    "    # Data containers for performance measure and model related data\n",
    "    data_return = data_return_init\n",
    "    frame_stamp = frame_stamp_init\n",
    "    avg_return = avg_return_init\n",
    "\n",
    "    # Train for a number of frames\n",
    "    t = t_init\n",
    "    e = e_init\n",
    "    policy_net_update_counter = policy_net_update_counter_init\n",
    "    t_start = time.time()\n",
    "    while t < NUM_FRAMES:\n",
    "        # Initialize the return for every episode (we should see this eventually increase)\n",
    "        G = 0.0\n",
    "\n",
    "        # Initialize the environment and start state\n",
    "        env.reset()\n",
    "        s = get_state(env.state())\n",
    "        s_cont = get_cont_state(env.continuous_state())\n",
    "    \n",
    "        is_terminated = False\n",
    "        while(not is_terminated) and t < NUM_FRAMES:\n",
    "            # Generate data\n",
    "            s_cont_prime, action, reward, is_terminated = world_dynamics(t, replay_start_size, num_actions, s_cont, env, policy_net)\n",
    "\n",
    "            sample = None\n",
    "            if replay_off:\n",
    "                sample = [transition(s_cont, s_prime, action, reward, is_terminated)]\n",
    "            else:\n",
    "                # Write the current frame to replay buffer\n",
    "                r_buffer.add(s_cont, s_cont_prime, action, reward, is_terminated)\n",
    "\n",
    "                # Start learning when there's enough data and when we can sample a batch of size BATCH_SIZE\n",
    "                if t > REPLAY_START_SIZE and len(r_buffer.buffer) >= BATCH_SIZE:\n",
    "                    # Sample a batch\n",
    "                    sample = r_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "            # Train every n number of frames defined by TRAINING_FREQ\n",
    "            if t % TRAINING_FREQ == 0 and sample is not None:\n",
    "                if target_off:\n",
    "                    train(sample, policy_net, policy_net, optimizer)\n",
    "                else:\n",
    "                    policy_net_update_counter += 1\n",
    "                    train(sample, policy_net, target_net, optimizer)\n",
    "\n",
    "            # Update the target network only after some number of policy network updates\n",
    "            if not target_off and policy_net_update_counter > 0 and policy_net_update_counter % TARGET_NETWORK_UPDATE_FREQ == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            G += reward.item()\n",
    "\n",
    "            t += 1\n",
    "\n",
    "            # Continue the process\n",
    "            s = s_cont_prime\n",
    "            s_cont = s_cont_prime\n",
    "\n",
    "        # Increment the episodes\n",
    "        e += 1\n",
    "\n",
    "        # Save the return for each episode\n",
    "        data_return.append(G)\n",
    "        frame_stamp.append(t)\n",
    "\n",
    "        # Logging exponentiated return only when verbose is turned on and only at 1000 episode intervals\n",
    "        avg_return = 0.99 * avg_return + 0.01 * G\n",
    "        if e % 1000 == 0:\n",
    "            logging.info(\"Episode \" + str(e) + \" | Return: \" + str(G) + \" | Avg return: \" +\n",
    "                         str(numpy.around(avg_return, 2)) + \" | Frame: \" + str(t)+\" | Time per frame: \" +str((time.time()-t_start)/t) )\n",
    "\n",
    "        # Save model data and other intermediate data if the corresponding flag is true\n",
    "        if store_intermediate_result and e % 1000 == 0:\n",
    "            torch.save({\n",
    "                        'episode': e,\n",
    "                        'frame': t,\n",
    "                        'policy_net_update_counter': policy_net_update_counter,\n",
    "                        'policy_net_state_dict': policy_net.state_dict(),\n",
    "                        'target_net_state_dict': target_net.state_dict() if not target_off else [],\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'avg_return': avg_return,\n",
    "                        'return_per_run': data_return,\n",
    "                        'frame_stamp_per_run': frame_stamp,\n",
    "                        'replay_buffer': r_buffer if not replay_off else []\n",
    "            }, output_file_name + \"_checkpoint\")\n",
    "\n",
    "    # Print final logging info\n",
    "    logging.info(\"Avg return: \" + str(numpy.around(avg_return, 2)) + \" | Time per frame: \" + str((time.time()-t_start)/t))\n",
    "        \n",
    "    # Write data to file\n",
    "    torch.save({\n",
    "        'returns': data_return,\n",
    "        'frame_stamps': frame_stamp,\n",
    "        'policy_net_state_dict': policy_net.state_dict()\n",
    "    }, output_file_name + \"_data_and_weights\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1a931b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--game GAME] [--output OUTPUT] [--verbose]\n",
      "                             [--loadfile LOADFILE] [--alpha ALPHA] [--save]\n",
      "                             [--replayoff] [--targetoff]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/zoe/Library/Jupyter/runtime/kernel-ea14590d-9b32-409b-8257-9bb7c8f40db2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--game\", \"-g\", type=str)\n",
    "    parser.add_argument(\"--output\", \"-o\", type=str)\n",
    "    parser.add_argument(\"--verbose\", \"-v\", action=\"store_true\")\n",
    "    parser.add_argument(\"--loadfile\", \"-l\", type=str)\n",
    "    parser.add_argument(\"--alpha\", \"-a\", type=float, default=STEP_SIZE)\n",
    "    parser.add_argument(\"--save\", \"-s\", action=\"store_true\")\n",
    "    parser.add_argument(\"--replayoff\", \"-r\", action=\"store_true\")\n",
    "    parser.add_argument(\"--targetoff\", \"-t\", action=\"store_true\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.verbose:\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # If there's an output specified, then use the user specified output.  Otherwise, create file in the current\n",
    "    # directory with the game's name.\n",
    "    if args.output:\n",
    "        file_name = args.output\n",
    "    else:\n",
    "        file_name = os.getcwd() + \"/\" + args.game\n",
    "\n",
    "    load_file_path = None\n",
    "    if args.loadfile:\n",
    "        load_file_path = args.loadfile\n",
    "\n",
    "    env = Environment(args.game)\n",
    "\n",
    "    print('Cuda available?: ' + str(torch.cuda.is_available()))\n",
    "    dqn(env, args.replayoff, args.targetoff, file_name, args.save, load_file_path, args.alpha)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1df14ab5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'object_NN.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File `'object_NN.py'` not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9f/64zk6_nx51j7cxqf5xdrfqb40000gn/T/ipykernel_1575/2959551949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'object_NN.py --game breakout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2349\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2350\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2351\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2352\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: File `'object_NN.py'` not found."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47660dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
